{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT \n",
    "\n",
    "## Bidirectional Encoder Representations from Transformers. \n",
    "\n",
    "#### Goal of BERT\n",
    "\n",
    "To pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.\n",
    "\n",
    "- Neural Machine Translation\n",
    "- Question Answering\n",
    "- Sentiment Analysis \n",
    "- Text summarization\n",
    "\n",
    "#### BERT Training \n",
    "\n",
    "- Pretrain BERT to understand language \n",
    "- Fine tune BERT to learn specific task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "There are 3 Passes\n",
    "\n",
    "##### Pretraining (Pass 1): \"What is language? What is context?\"\n",
    "\n",
    "__Masked Language Model (MLM)__\n",
    "\n",
    "ex. The [MASK1] brown fox [MASK2] over the lazy dog. \n",
    "\n",
    "__Next Sentence Prediction (NSP)__"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
